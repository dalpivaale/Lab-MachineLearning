7+8
2+2
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
logis0 <- glm(diabetes~.,family=binomial,data=pima.data)#qua faccio la lostici regression
summary(logis0)
fish <- read.table("fishcatch.dat.txt")
fish <- read.table("fishcatch.dat")
names(fish) <- c("obs", "species", "weight","length1", "length2", "length3", "heightpct","widthpct","sex")#diverse speci3 di pesci con 3 tipi di lunghezza ciascuno, obs è il numero di osservazioni, weight il peso
fish <- read.table("fishcatch.dat")
fish <- read.table("fishcatch.dat.txt")
fish <- read.table("fishcatch.txt")
names(fish) <- c("obs", "species", "weight","length1", "length2", "length3", "heightpct","widthpct","sex")#diverse speci3 di pesci con 3 tipi di lunghezza ciascuno, obs è il numero di osservazioni, weight il peso
fish <- read.table("fishcatch.dat")
names(fish) <- c("obs", "species", "weight","length1", "length2", "length3", "heightpct","widthpct","sex")#diverse speci3 di pesci con 3 tipi di lunghezza ciascuno, obs è il numero di osservazioni, weight il peso
fish <- read.table("fishcatch.txt")
?as.factor
?detach
?attach
fish <- read.table("fishcatch.dat.txt")
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
setwd("Users\dpale\Desktop\Lab_MachineLearning")
data("PimaIndiansDiabetes2", package = "mlbench")#il package mlbench contiene il dataset PrimaIndianDiabetes2
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
# Naive Bayes
library(e1071) #carico il package nel workspace
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
source("C:/Users/dpale/Desktop/Lab_MachineLearning/es1_lab5_LogisticRegrassion.R")
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
summary(ct)
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete.dat", stringsAsFactors=TRUE)
#es1 lab5 Logistic regression
ct <- read.table("BreastCancer_complete", stringsAsFactors=TRUE)
### Read data
ct <- read.table("CellType.dat", stringsAsFactors=TRUE) #Legge un file in formato tabella e crea da esso un dataframe.i vettori di carartteri sono dei fattori quindi metto TRUE
#lab9 Esercizi decision tree
fish <- read.table("bbd_data.dat")
load("C:/Users/dpale/Desktop/Lab_MachineLearning/bbd_data.dat")
fish <- read.table("fishcatch.dat.txt")
data<- read.table("ALL_data.data", stringsAsFactors=T)
plot(cars)
load("C:/Users/dpale/Desktop/LabR/train.csv")
load("C:/Users/dpale/Desktop/LabR/test.csv")
data<- read.table("raisin.dat", stringsAsFactors=T)
library(NbClust)
library(factoextra)
library(cluster)
no.diab.data <- scale(data[,-8])#Remove last column about Class
k.diab2 <- kmeans(no.diab.data, 2)
k.diab2$centers
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
train <- read.csv("train.csv", header = TRUE)
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
set.seed(3)
idx = sample(nrow(pima.data),250)
data<- read.table("ALL_data.data", stringsAsFactors=T)
train <- read.csv("train.csv", header = TRUE)
train <- read.csv(train.csv, header = TRUE)
train <- read.csv(train.csv, header = TRUE)
data("PimaIndiansDiabetes2", package = "mlbench") # you must have the mlbench package installed
dim(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
dim(pima.data)
str(pima.data)
head(pima.data)
plot(glucose~age, data=pima.data, pch=4*as.numeric(pima.data$diabetes), col=as.numeric(pima.data$diabetes))
plot(mass~pedigree, data=pima.data, pch=4*as.numeric(pima.data$diabetes), col=as.numeric(pima.data$diabetes))
plot(insulin~pressure, data=pima.data, pch=4*as.numeric(pima.data$diabetes), col=as.numeric(pima.data$diabetes))
plot(pressure~diabetes,data=pima.data)
plot(insulin~diabetes,data=pima.data)
### k nearest neighbor
library(FNN) # you must have the FNN package installed
set.seed(3)
idx = sample(nrow(pima.data),250)
tst = pima.data[-idx,]#gli altri 250
trn = pima.data[idx,] #250
tst = pima.data[-idx,]#gli altri 250
summary(tst)
chance <- table(tst$diabetes)/nrow(tst)
table(tst$diabetes)
table(tst$diabetes)
chance
#?knn
pred <- knn(trn[,1:8],tst[,1:8],trn$diabetes,k=1) # not scaled!
pred
?knn
accuracy = sum(pred==tst$diabetes)/nrow(tst)
accuracy
pred==tst$diabetes
sum(pred==tst$diabetes)
nrow(tst)
TRUE+TRUE
FALSE+FALSE
pred <- knn(scale(trn[,1:8]),scale(tst[,1:8]),trn$diabetes,k=1) #scale!
accuracy = sum(pred==tst$diabetes)/nrow(tst)
accuracy
pred
pred
pred <- knn(scale(trn[,1:8]),scale(tst[,1:8]),trn$diabetes,k=1) #scale!
pred
# confusion matrix
x<- table(pred,tst$diabetes)
x
table(tst$diabetes)
sum(diag(x))/sum(sum(x)) #accuracy
# Vary k, the number of nearest neighbors
KK=nrow(tst)
accuracy=rep(0,KK)
for(k in 1:KK){
pred <- knn(scale(trn[,1:8]),scale(tst[,1:8]),trn$diabetes,k=k)
accuracy[k] = sum(pred==tst$diabetes)/nrow(tst)
}
plot(1:KK,accuracy,type='b',ylim=c(0.65,0.8),xlab='k')
?knn
# Vary k, the number of nearest neighbors
KK=nrow(tst)
KK
accuracy=rep(KK)
accuracy
for(k in 1:KK){
pred <- knn(trn[,1:8],tst[,1:8],trn$diabetes,k=k)
accuracy[k] = sum(pred==tst$diabetes)/nrow(tst)
}
lines(1:KK,accuracy,type='l',col='red')
rep(KK)
?rep
rep(5)
a <- rep(5)
a <- rep(0,7)
data("PimaIndiansDiabetes2", package = "mlbench")#il package mlbench contiene il dataset PrimaIndianDiabetes2
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
# Naive Bayes
library(e1071) #carico il package nel workspace
data("PimaIndiansDiabetes2", package = "mlbench")#il package mlbench contiene il dataset PrimaIndianDiabetes2
pima.data <- na.omit(PimaIndiansDiabetes2) # remove rows with missing data
# Naive Bayes
library(e1071) #carico il package nel workspace
diabetes~
diabetes~.
nb1 <- naiveBayes(diabetes~.,data= PimaIndiansDiabetes2) # works with NAs! ~.confronta stima diabates usando tutte le variabili, costruisce il modello tenendo conto di tutte le altre variabili
nb2 <- naiveBayes(diabetes~.,data= pima.data) #tilde e punto indica che diabetes dipende da tutte le features
table(predict(nb1,newdata=PimaIndiansDiabetes2), PimaIndiansDiabetes2$diabetes)#predict predice il dataset Pima... usando il classificatore nb1
table(predict(nb1,newdata=PimaIndiansDiabetes2), PimaIndiansDiabetes2$diabetes)#predict predice il dataset Pima... usando il classificatore nb1
table(predict(nb2,newdata=PimaIndiansDiabetes2), PimaIndiansDiabetes2$diabetes)#un pò meglio dell'altro ma fanno schifo tutti e due un pò
library(naivebayes) # yet another package - includes Poisson distribution
nb3<- naive_bayes(diabetes~., data=pima.data)
#pima.data.int <- pima.data
#pima.data.int$pregnant <- as.integer(pima.data.int$pregnant)
#nb3i<- naive_bayes(diabetes~., data=pima.data.int, usepoisson=T)
par(mfrow=c(2,5))#costruisce una window con subplot c(2,5) vettore due righe e 5 colonne
plot(nb3)
nb3
gluc <- nb3$tables[[2]]  # [[i]] to select in lists
gluc
#par(mfrow=c(1,2))
hist(pima.data$glucose[pima.data$diabetes=="neg"],freq=F,main="neg",xlab="glucose",xlim=c(50,200))#isogramma dei valori del glucosio
lines(dnorm(1:200,mean=gluc[1,1],sd=gluc[2,1]),col="blue")
hist(pima.data$glucose[pima.data$diabetes=="pos"],freq=F,main="pos",xlab="glucose",xlim=c(50,200))#mette una linea di colore blu
lines(dnorm(1:200,mean=gluc[1,2],sd=gluc[2,2]),col="blue")
#par(mfrow=c(1,2))
hist(pima.data$glucose[pima.data$diabetes=="neg"],freq=F,main="neg",xlab="glucose",xlim=c(50,200))#isogramma dei valori del glucosio
lines(dnorm(1:200,mean=gluc[1,1],sd=gluc[2,1]),col="blue")
hist(pima.data$glucose[pima.data$diabetes=="pos"],freq=F,main="pos",xlab="glucose",xlim=c(50,200))#mette una linea di colore blu
lines(dnorm(1:200,mean=gluc[1,2],sd=gluc[2,2]),col="blue")
table(predict(nb2,pima.data[,1:8]),pima.data$diabetes)
table(predict(nb3,pima.data[,1:8]),pima.data$diabetes)
#?train
trn.ctrl <- trainControl(method="LOOCV")
# TNR = x[1,1]/sum(x[,1])
# TNR # 0.82
library(caret)
# TNR = x[1,1]/sum(x[,1])
# TNR # 0.82
library(caret)
confusionMatrix(pred.nb,tst$diabetes, positive="pos")#necessità del package caret
#accuracy 0.739 non fantastica, no information rate 0.69 altino, kappa value 0.37 schifetto
## Cross-validation
library(caret)
#?train
trn.ctrl <- trainControl(method="LOOCV")
trn.ctrl.10 <- trainControl(method="cv",number=10)
?trainControl
nb.train <- train(diabetes~., data=trn,method="nb",trControl=trn.ctrl.10)#fa una 10 fold cross validation costruendo il nostro modello. tutti e 10 i modelli sono in nb.train
#?train
trn.ctrl <- trainControl(method="LOOCV")
trn.ctrl.10 <- trainControl(method="cv",number=10)
nb.train <- train(diabetes~., data=trn,method="nb",trControl=trn.ctrl.10)#fa una 10 fold cross validation costruendo il nostro modello. tutti e 10 i modelli sono in nb.train
set.seed(3)
idx = sample(nrow(pima.data),250)#divido il dataset in training e testset
trn = pima.data[idx,]
tst = pima.data[-idx,]
#?train
trn.ctrl <- trainControl(method="LOOCV")
trn.ctrl.10 <- trainControl(method="cv",number=10)
nb.train <- train(diabetes~., data=trn,method="nb",trControl=trn.ctrl.10)#fa una 10 fold cross validation costruendo il nostro modello. tutti e 10 i modelli sono in nb.train
nb.train
nb.train$bestTune
nb.final <- nb.train$finalModel #Si prende il modello migliore diciamo, quello con accuracy più alta che è tipo 0.78-0.79
pred.final<-predict(nb.final,tst)$class #predizione della classe col nuovo classificatore ottenuto con la cross validation
x <- table(pred.final,tst$diabetes)
confusionMatrix(x)#accuratezza è 0.70 fa ancora un pò schifo. #comparo le predizioni con la realtà
predict(nb.final,tst)
pred.nb<-predict(nb,newdata=tst)#predizione del testset col classificatore appena costruito
set.seed(3)
idx = sample(nrow(pima.data),250)#divido il dataset in training e testset
trn = pima.data[idx,]
tst = pima.data[-idx,]
#?naiveBayes
nb <- naiveBayes(diabetes~.,data=trn)
pred.nb<-predict(nb,newdata=tst)#predizione del testset col classificatore appena costruito
x=table(pred.nb,tst$diabetes)
x
predict(nb,newdata=tst)
x=table(pred.nb,tst$diabetes)
x
accuracy=sum(diag(x))/nrow(tst)
accuracy #0.74
library(car)
View(SLID)
tab_oss <- table(SLID$language,SLID$sex)
tab_oss
ris<- chisq.test(tab_oss)
ris
ris$expected
### Acute lymphoblastic leukemia (ALL) data
data<- read.table("ALL_data.data", stringsAsFactors=T)
dat <- data[,1:(ncol(data)-1)]	# the gene expression
setwd("C:/Users/dpale/Desktop/Lab_MachineLearning")
### Acute lymphoblastic leukemia (ALL) data
data<- read.table("ALL_data.data", stringsAsFactors=T)
dat <- data[,1:(ncol(data)-1)]	# the gene expression
cl <- as.factor(data$cl) 					# the true cell type
dim(dat)
# create new data set with only the 100 genes with highest variance
genes.var<-apply(dat,2,var) 					# variance of genes, see ?apply
genes.var.select <- order(-genes.var)[1:100] 	# see ?order
dat.s <- dat[,genes.var.select]
dim(dat.s)
library(NbClust)
library(factoextra)
library(cluster)
d <- dist(dat)
d <- dist(dat)
image(as.matrix(d))
?pam
?par
